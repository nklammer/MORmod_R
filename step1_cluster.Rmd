---
title: "Cluster and Analyze"
author: "Noah Klammer"
date: "4/27/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## Clear global env and report

```{r include=FALSE}
rm(list = ls())
gc()
```

# Intro

In this instance, I am using Zone Ideal Air Loads extracted from an EnergyPlus `in.idf` with the weather file `USA_CA_Los.Angeles.Intl.AP.722950_TMY3.ddy`. The Zone Ideal Load Air System ("ILAS") numerical `colnames` were `str_replace`d to match the `Name` field of the `Zone` object in the predecessor file `0_RVESO.Rmd`. Loads were normalized by zone floor area. That file also exported a R data file object `ILAS.Rda`. In this file `1_CLUS.Rmd` we will import both Ideal Load Air System data and weather data via the connector package `eplusr`. We will visualize this data for exploration. We will create report figures. We will attempt clustering of the ideal air loads for a subset of 8760 observations. Note that the nth observation in the `hour` variable corresponds to the average of the period from (n-1)th hour to nth hour.

## Import EPW

The R package `eplusr` contains useful functions and an `Epw` object for easily reading and writing weather data in the .epw file format.

```{r include=FALSE}
library(eplusr)
epw <- eplusr::Epw$new("USA_CA_Los.Angeles.Intl.AP.722950_TMY3.epw")

# good info
epw$typical_extreme_period()
paste0("The number of time intervals per hour is ",epw$interval(),".")
paste(colnames(epw$data()), collapse = ", ")
epw$data() %>%
  select(month, day, hour, dry_bulb_temperature)
```

## Import Rda of Zone Ideal Loads Air System

The responsible model for this data resides in the project folder `MORmod_mf/sddc_osw_multifamily/*/in/run`.

```{r import, message=FALSE, warning=FALSE}
load(file = "ilas_nodhw.Rda")
```

### Select four days to cluster in 24-d vector

I have selected the following dates of interest: January 21, April 21, August 21, and October 21 from preliminary visualization in the Climate Consultant desktop app. I create two vectors, `days_of_interest = c(21)` and `months_of_interest = c(1,4,8,10)` then use a pipe `filter` to get my result. As of now there are two separate dataframes, `epw` for weather and `ILAS` for Zone Ideal Loads.

```{r}
# change to numeric format of `day` and `month`
days_of_interest <- c(21); months_of_interest <- c(1,4,8,10)

sel_days_ILAS <- ilas_nodhw %>%
  filter(day %in% days_of_interest, month %in% months_of_interest)

```

```{r}
sel_days_epw <- epw$data() %>%
  select(month, day, hour, dry_bulb_temperature) %>%
  filter(day %in% days_of_interest, month %in% months_of_interest)
```

After that filter is done, I assert that it is `r nrow(sel_days_ILAS)==96` that I have 96 (4 x 24) observations.


# Visualizations 

Under this section I have code that opens the `grDevices` connection and writes custom figures to bitmap files in the containing folder. The image outputs are shown below.

### faceted ggplot for select 4 days

```{r RUN FOR 4 IMAGE, eval=FALSE, include=FALSE}
library(ggplot2)
# Jan, Apr, Aug, Oct

x <- sel_days_epw

# set resolution
png("4days.png", units = "in", width = 8, height = 5, res = 500)

# have to tell aes that the variable is a discrete `factor`
p <- ggplot(x, aes(x = hour, y = dry_bulb_temperature, color = factor(month))) + geom_smooth(se = F, span = 0.3)

# new facet label names for month variable
month.labs <- c("January 21","April 21","August 21","October 21")
names(month.labs) <- months_of_interest

p + facet_grid(rows = vars(month), labeller = labeller(month = month.labs)) + scale_color_discrete(name = "Month", labels = month.name[months_of_interest]) + scale_x_continuous(name = "Hour", breaks = c(1,6,12,18,24)) + theme_bw() + theme(legend.position = "none", axis.title.y = element_text(margin = margin(r=10))) + labs(y = "Dry Bulb Temperature (C)")

dev.off()
```

Dry bulb temperature versus hour for four select days in the typical Los Angeles meteorological year: ![](4days.png)

### faceted ggplot for 365 days

```{r eval=FALSE, include=FALSE}
# make unique day index 1..365
d <- nrow(ilas_nodhw)/24
d_ind <- rep(1:d, each = 24)

# alternative 2 with iteration
# c_vec <- vector(mode = "numeric", nrow(ilas_nodhw)) # instantiate
# for (i in length(c_vec)) { # control flow
#   if (logic) {i <<- i + 1} # iterator
# }

```


```{r RUN FOR 365 IMAGE, eval=FALSE, include=FALSE}
library(ggplot2)
# Jan 31 days
sel_days_epw <- cbind(d_ind, epw$data()) %>%
  select(d_ind, month, day, hour, dry_bulb_temperature) %>%
  filter(month %in% c(1:12))

x <- sel_days_epw

# set resolution
png("365.png", units = "in", width = 8, height = 5, res = 500)

# custom color palette named vector
colorv <- rep( 
  RColorBrewer::brewer.pal(8, name = "Dark2"),
  length.out = 12)
names(colorv) <- 1:12

# have to tell aes that the variable is a discrete `factor`
p <- ggplot(x, aes(x = hour, y = dry_bulb_temperature, color = factor(month))) + geom_smooth(se = F, span = 0.35)

p + facet_wrap(~d_ind) + scale_color_manual(name = "Month", labels = month.abb, values = colorv) + scale_x_discrete(labels = NULL) + scale_y_discrete(labels = NULL) + theme(strip.background = element_blank(), strip.text.x = element_blank(), axis.title.y = element_text(margin = margin(r=12))) + labs(x = NULL, y = "Drybulb temperature")

dev.off()
```

Dry bulb temperature versus time of the full 365 days of the typical meteorological year for Los Angeles: ![](365.png)



# Clustering

### TODO - take ilas_nodhw from one day, January 21

You might think that they would only be heating loads on the winter extreme day, but in this model and climate, we find that there is more building cooling load [J] than heating load.

Traditionally, we would remove columns with zero variance as they are unhelpful in the sense of regression. However, in clustering we may want to leave them in.

```{r}
days_of_interest <- c(21); months_of_interest <- c(8)

sel_days_ILAS <- ilas_nodhw %>%
  filter(day %in% days_of_interest, month %in% months_of_interest)

# transpose col variables to observation rows
# a matrix-like object must have the same class for all cols
# for a transpose
# colnames(sel_days_ILAS)

# t
# need col names
t <- as.data.frame(t(sel_days_ILAS))
colnames(t) <- sel_days_ILAS$hour

# "the standard deviation is zero"?
# visdat::vis_cor(sel_days_ILAS)

# let's find which columns have zero variance
zv <- which(apply(sel_days_ILAS, 2, var) == 0)
zero_var_zones <- colnames(sel_days_ILAS)[zv]

# get zones that str match with "cooling"
c <- grep("*\\Dooling", zero_var_zones, value = TRUE)
# get zones that str match with "heating"
h <- grep("*\\Deating", zero_var_zones, value = TRUE)

# extracts zone name logic
c <- str_extract(c,"(?<=SYSTEM\\s).*(?=\\:)")
h <- str_extract(h,"(?<=SYSTEM\\s).*(?=\\:)")

# are there any zones with neither heating nor cooling?
intersect(c,h)
```

The longer we work with this data set, the more clear it is that many zones have zero variance. We see that `r length(zero_var_zones)` out of `r length(sel_days_ILAS)` zones have zero variance for this temporal range.

In `r month.name[months_of_interest]`, we find that there are `r length(c)` zones with no cooling load and `r length(h)` zones with no heating load. We assert that `r length(intersect(c,h))` have neither cooling nor heating load.

What's going on? No cooling/heating energy in `CORRIDOR_2`, `CORRIDOR_3`, or `CORRIDOR_4`. They must be unconditioned.


### Transpose and cluster

Let's introduce the `apcluster` package which is an implementation of Frey and Dueck's popular Affinity Propagation method for passing messages between pairs of data. I would make sure to reference the [inspiration paper](https://doi.org/10.1080/19401493.2017.1410572), the [R package](https://doi.org/10.1093/bioinformatics/btr406), and the [original method's](https://doi.org/10.1126/science.1136800) publication.



```{r message=FALSE, warning=FALSE, include=FALSE}
# select one zone var "Cooling Energy [J/m^2] Hourly"
# select four hours (can be non-consecutive)

# get the maximum cooling value hour
# hint: it's not DDY


# take 99.6% percentile cooling
# greatest cooling load is "2_BDRM_1_3"
a <- ilas_nodhw %>%
  arrange(desc(`ZONE HVAC IDEAL LOADS AIR SYSTEM 2_BDRM_1_3: Cooling Energy [J/m^2](Hourly)`))
max_cool_hour <- a$hour[1]; max_cool_day <- a$day[1]; max_cool_month <- a$month[1];

# take 99.6% percentile heating
# greatest heating load is "2_BDRM_3_4"
a <- ilas_nodhw %>%
  arrange(desc(`ZONE HVAC IDEAL LOADS AIR SYSTEM 2_BDRM_3_4: Heating Energy [J/m^2](Hourly)`))
max_heat_hour <- a$hour[1]; max_heat_day <- a$day[1]; max_heat_month <- a$month[1];

#########
max_heat <- ilas_nodhw %>%
  select(contains("Heating")) %>%
  sapply(max)

max_heat_zone <- names(which(max_heat==max(max_heat)))

#########

max_cool <- ilas_nodhw %>%
  select(contains("Cooling")) %>%
  sapply(max)

max_cool_zone <- names(which(max_cool==max(max_cool)))

#########

max_heat <- ilas_nodhw %>%
  select(contains("Heating")) %>%
  summarize(across(,max))

# drop temporal columns to prepare for clustering  
sub <- subset(ilas_nodhw, select = -c(day, hour, minute, month))

```

To reduce the parameter space across which we cluster, we'll determine which hours of the year represent the building's thermal response curve. The zone that has the highest area-normalized cooling load is ``r max_cool_zone`` (needs to be normalized). This peak cooling load occurs at hour `r max_cool_hour` on `r paste0(max_cool_month,"/",max_cool_day)`. The zone that has the highest area-normalized heating load is ``r max_heat_zone`` (needs to be normalized). This peak heating load occurs at hour `r max_heat_hour` on `r paste0(max_heat_month,"/",max_heat_day)`.

To start, we'll take the two aforementioned peak hours as rows as a subset of our annual data. We'll keep all the thermal zones.

```{r}

# get row for max cooling peak -- 
# can get max heating in this way also
max_cool_row <- which(max(max_cool)==ilas_nodhw[[max_cool_zone]])
max_heat_row <- which(max(max_heat)==ilas_nodhw[[max_heat_zone]])

# add shoulder days
# should represent periods of peak and off-peak schedules
# weekday vs weekend (calendar year 2019)
ilas_nodhw %>%
  select(month,day,hour,`ZONE HVAC IDEAL LOADS AIR SYSTEM 2_BDRM_3_4: Heating Energy [J/m^2](Hourly)`) %>%
  arrange(desc(`ZONE HVAC IDEAL LOADS AIR SYSTEM 2_BDRM_3_4: Heating Energy [J/m^2](Hourly)`)) %>%
  slice(seq(1,10))

# like 05:00 March 19 (wd), and 22:00 Sept 28 (we)
# need high heating day during operation - 10:00 January 7 (wd)
# need high cooling day outside of operation - 08:00 August 18 (we)
row_index <- function(h,d,m)
  {return(which(ilas_nodhw$hour==h & ilas_nodhw$day==d & ilas_nodhw$month==m))}


###########

```

```{r}
library(apcluster)

# choose subselection of hours (observations)
sub_rows <- c(max_heat_row, row_index(5,19,3), max_cool_row)

# select only cooling load vars
a <- ilas_nodhw %>%
  select(contains("Cooling")) %>%
  slice(sub_rows)

# transpose, observations are now variable, vice versa
a <- as.data.frame(t(a))

# add chars to var names **Zone Cooling [J/m^2]**
colnames(a) <- paste(colnames(a),c("Zone Heating [J/m^2]", rep("Zone Cooling [J/m^2]",2)))
# drop all chars but ZONE NAME from observations
rownames(a) <- str_extract(rownames(a),"(?<=SYSTEM\\s).+(?=\\:)")

# need to take transpose
# observations are zones, parameters are select hours
apcluster::apcluster(negDistMat(r=2), a)
apres1 <- apcluster::apcluster(negDistMat(r=2), a)

# illustrative purposes only
# use apcluster's plot(apdata, data) scatterplot overlay
# need to alter t(a) so variables have better names
apcluster::plot(apres1, a)

# use apcluster::aggExCluster() for agglomerative dendogram
aggres1 <- aggExCluster(negDistMat(r=2), a)
plot(aggres1)

```

What's up with `1_BDRM_6_3`? Investigate this.

### make nice graphic export

```{r eval=FALSE, include=FALSE}
# set resolution
png("3var_cluster.png", units = "in", width = 8, height = 5, res = 500)

apcluster::plot(apres1, a)

dev.off()
```

![](3var_cluster.png)

### TODO - transform heating loads into negative cooling loads

The climate in coastal California is cooling-dominated so there is a certain logic to casting heating loads as the opposite of cooling loads. For future analysis, combine the heating and cooling loads into one variable. Since this model has been quality checked, there should never be hours with both heating and cooling.

### TODO - get typical days

Post analysis, during writing, I investigated further into why there is such a strong outlier in the zone '1_BDRM_6_3' which is the sixth one-bedroom unit on the third floor of the model. It turns out that it is an unfortunate coincidence that the three representative hours I picked for the annual simulation just happened to include the peak cooling time for that zone as confirmed by the `eplustbl.html` report file.

In the future we can address this risk of outlier by including more parameters for the clustering algorithm (like the operating hours for the whole month of August). Another approach would be to use the `eplusr` package's `typical_extreme_period()` function to find out the most typical (least extreme) days in the simulation year.



<br><br><br>