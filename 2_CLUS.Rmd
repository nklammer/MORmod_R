---
title: "Cluster and Analyze 2"
author: "Noah Klammer"
date: "6/04/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## Clear global env and report

```{r include=FALSE}
rm(list = ls())
gc()
```

# Intro

This is my second attempt at clustering of zones using the ideal air loads output variable.

In this instance, I am using Zone Ideal Air Loads extracted from an EnergyPlus `in.idf` with the weather file `USA_CA_Los.Angeles.Intl.AP.722950_TMY3.ddy`. The Zone Ideal Load Air System ("ILAS") numerical `colnames` were `str_replace`d to match the `Name` field of the `Zone` object in the predecessor file `0_RVESO.Rmd`. Loads were normalized by zone floor area. That file also exported a R data file object `ILAS.Rda`. In this file `1_CLUS.Rmd` we will import both Ideal Load Air System data and weather data via the connector package `eplusr`. We will visualize this data for exploration. We will create report figures. We will attempt clustering of the ideal air loads for a subset of 8760 observations. Note that the nth observation in the `hour` variable corresponds to the average of the period from (n-1)th hour to nth hour.

## Import EPW

The R package `eplusr` contains useful functions and an `Epw` object for easily reading and writing weather data in the .epw file format.

```{r include=FALSE}
library(eplusr)
epw <- eplusr::Epw$new("USA_CA_Los.Angeles.Intl.AP.722950_TMY3.epw")

# good info
epw$typical_extreme_period()
paste0("The number of time intervals per hour is ",epw$interval(),".")
paste(colnames(epw$data()), collapse = ", ")
epw$data() %>%
  select(month, day, hour, dry_bulb_temperature)
```

## Import Rda of Zone Ideal Loads Air System

```{r import, message=FALSE, warning=FALSE}
load(file = "ILAS.Rda")
```

### Select N hours to cluster in N-space

Previously, based on the advice of Zahra Fallahi, I chose four select days on which to base my first trial of clustering. Based on the results from the first trial, I saw that even good intentions were not successful in preventing bias in the clustering. Specifically, one of the hours I chose as a parameter happened to coincide with the annual peak cooling load for one of my zones ("1_BDRM_6_3"). For this second trial, I will try to be more cognizant of the most typical (least extreme) and most extreme (least typical) weather periods of the simulation year.

### Get typical and extreme days

The following method leverages the `eplusr` package to identify the most typical weeks of the four North American seasons - Summer, Winter, Autumn, and Spring.

I have used data filtering to the greatest extent but I worry that the number of observations is still too high for later clustering. Remember, zones are my points so hourly observations are actually my parameters after the necessary transform.

```{r include=FALSE}

view_me <- epw$typical_extreme_period()
names <- epw$typical_extreme_period()$name

epw_strings <- c(
  "Summer - Week Nearest Average Temperature For Period",
  "Winter - Week Nearest Average Temperature For Period",
  "Autumn - Week Nearest Average Temperature For Period",
  "Spring - Week Nearest Average Temperature For Period"
)

# names[c(2,4:6)]
start_dates <- as.character( epw$typical_extreme_period()$start_day[c(2,4:6)] )
end_dates <- as.character( epw$typical_extreme_period()$end_day[c(2,4:6)] )
# => [1] " 08/21" " 4/18" "11/28" " 6/25"

# look ahead regexp for month chars
months_num <- as.numeric( str_extract(start_dates,"\\d+(?=/)") )

# look behind regexp for day chars
start_days <- as.numeric( str_extract(start_dates,"(?<=/)\\d+") )
end_days <- as.numeric( str_extract(end_dates,"(?<=/)\\d+") )

months_of_interest <- months_num # from typical weeks above
hours_of_interest <- 8:20 # hours of operation


num_obs <- ILAS %>%
  filter(month %in% months_of_interest) %>% # 2,904 rows
  filter(hour %in% hours_of_interest) %>%
  nrow() # 1,573 rows
```

The typical weeks for Summer: ``r month.abb[months_num][1]`` week starting ``r start_days[1]``; Winter: ``r month.abb[months_num][2]`` week starting ``r start_days[2]``; Autumn: ``r month.abb[months_num][3]`` week starting ``r start_days[3]``; Spring: ``r month.abb[months_num][4]`` week starting ``r start_days[4]``.

After filtering for only months ``r month.name[months_num]`` and operating hours 8:00 AM to 8:00 PM there still remains ``r num_obs`` observations. I believe we need a more targeted subselection of the dataframe. In the next code chunk I implement a subselection operation where only the four weeks of interest to us are selected. This required quite a bit more custom code but hopefully it is reusable.

```{r}
# seq(start_days, end_days) but vectorized
x <- cbind(start_days, end_days)
f_range <- function(x, from_, to_) seq(x[from_], x[to_])
x <- apply(x, 1, f_range, from_ = "start_days", to_ = "end_days")
colnames(x) <- as.character(months_num)

# just remember that %in% is the
# vectorized version of '=='

# initialize vector to write for loop to
idx = vector(mode = "numeric")
for (set in colnames(x)) {
  output <- which(ILAS$day %in% x[,set]
        & ILAS$month == as.numeric(set))
  idx <- append(idx,output)
  idx <- sort(idx)
}
# ILAS[idx,] #=> 672 rows

sel_days_ILAS <- ILAS[idx,] %>%
  filter(hour %in% hours_of_interest) #=> 364 rows
```


After that custom filter is done, I have `r nrow(sel_days_ILAS)` observations.

# Clustering

You might think that they would only be heating loads on the winter extreme day, but in this building type and climate, we find that there is more building cooling load [J] than heating load even during winter.

Traditionally, we would remove columns with zero variance as they are unhelpful in the sense of regression. However, in clustering we may want to leave them in.


```{r}
days_of_interest <- c(21); months_of_interest <- c(8)

sel_days_ILAS <- ILAS %>%
  filter(day %in% days_of_interest, month %in% months_of_interest)

# transpose col variables to observation rows
# a matrix-like object must have the same class for all cols
# for a transpose
# colnames(sel_days_ILAS)

# t
# need col names
t <- as.data.frame(t(sel_days_ILAS))
colnames(t) <- sel_days_ILAS$hour

# "the standard deviation is zero"?
# visdat::vis_cor(sel_days_ILAS)

# let's find which columns have zero variance
zv <- which(apply(sel_days_ILAS, 2, var) == 0)
zero_var_zones <- colnames(sel_days_ILAS)[zv]

# get zones that str match with "cooling"
c <- grep("*\\Dooling", zero_var_zones, value = TRUE)
# get zones that str match with "heating"
h <- grep("*\\Deating", zero_var_zones, value = TRUE)

# extracts zone name logic
c <- str_extract(c,"(?<=SYSTEM\\s).*(?=\\:)")
h <- str_extract(h,"(?<=SYSTEM\\s).*(?=\\:)")

# are there any zones with neither heating nor cooling?
intersect(c,h)
```

The longer we work with this data set, the more clear it is that many zones have zero variance. We see that `r length(zero_var_zones)` out of `r length(sel_days_ILAS)` zones have zero variance for this temporal range.

In `r month.name[months_of_interest]`, we find that there are `r length(c)` zones with no cooling load and `r length(h)` zones with no heating load. We assert that `r length(intersect(c,h))` have neither cooling nor heating load.

What's going on? No cooling/heating energy in `CORRIDOR_2`, `CORRIDOR_3`, or `CORRIDOR_4`. They must be unconditioned.


### Transpose and cluster

Let's introduce the `apcluster` package which is an implementation of Frey and Dueck's popular Affinity Propagation method for passing messages between pairs of data. I would make sure to reference the [math paper](https://doi.org/10.1080/19401493.2017.1410572), the [R package](https://doi.org/10.1093/bioinformatics/btr406), and the [original method's](https://doi.org/10.1126/science.1136800) publication.



```{r eval=FALSE}
# select one zone var "Cooling Energy [J/m^2] Hourly"
# select four hours (can be non-consecutive)


# greatest cooling load is "2_BDRM_1_3"
a <- ILAS %>%
  arrange(desc(`ZONE HVAC IDEAL LOADS AIR SYSTEM 2_BDRM_1_3: Cooling Energy [J/m^2](Hourly)`))
max_cool_hour <- a$hour[1]; max_cool_day <- a$day[1]; max_cool_month <- a$month[1];

# take 99.6% percentile heating
# greatest heating load is "2_BDRM_3_4"
a <- ILAS %>%
  arrange(desc(`ZONE HVAC IDEAL LOADS AIR SYSTEM 2_BDRM_3_4: Heating Energy [J/m^2](Hourly)`))
max_heat_hour <- a$hour[1]; max_heat_day <- a$day[1]; max_heat_month <- a$month[1];

#########
max_heat <- ILAS %>%
  select(contains("Heating")) %>%
  sapply(max)

max_heat_zone <- names(which(max_heat==max(max_heat)))

#########

max_cool <- ILAS %>%
  select(contains("Cooling")) %>%
  sapply(max)

max_cool_zone <- names(which(max_cool==max(max_cool)))

#########

max_heat <- ILAS %>%
  select(contains("Heating")) %>%
  summarize(across(,max))

# drop temporal columns to prepare for clustering  
sub <- subset(ILAS, select = -c(day, hour, minute, month))

```


To start, we'll take the two aforementioned peak hours as rows as a subset of our annual data. We'll keep all the thermal zones.

```{r eval=FALSE, include=TRUE}

# get row for max cooling peak -- 
# can get max heating in this way also
max_cool_row <- which(max(max_cool)==ILAS[[max_cool_zone]])
max_heat_row <- which(max(max_heat)==ILAS[[max_heat_zone]])

# add shoulder days
# should represent periods of peak and off-peak schedules
# weekday vs weekend (calendar year 2019)
ILAS %>%
  select(month,day,hour,`ZONE HVAC IDEAL LOADS AIR SYSTEM 2_BDRM_3_4: Heating Energy [J/m^2](Hourly)`) %>%
  arrange(desc(`ZONE HVAC IDEAL LOADS AIR SYSTEM 2_BDRM_3_4: Heating Energy [J/m^2](Hourly)`)) %>%
  slice(seq(1,10))

# like 05:00 March 19 (wd), and 22:00 Sept 28 (we)
# need high heating day during operation - 10:00 January 7 (wd)
# need high cooling day outside of operation - 08:00 August 18 (we)
row_index <- function(h,d,m)
  {return(which(ILAS$hour==h & ILAS$day==d & ILAS$month==m))}



###########


```

```{r eval=FALSE, include=TRUE}
library(apcluster)

# choose subselection of hours (observations)
sub_rows <- c(max_heat_row, row_index(5,19,3), max_cool_row)

# select only cooling load vars
a <- ILAS %>%
  select(contains("Cooling")) %>%
  slice(sub_rows)

# transpose, observations are now variable, vice versa
a <- as.data.frame(t(a))

# add chars to var names **Zone Cooling [J/m^2]**
colnames(a) <- paste(colnames(a),c("Zone Heating [J/m^2]", rep("Zone Cooling [J/m^2]",2)))
# drop all chars but ZONE NAME from observations
rownames(a) <- str_extract(rownames(a),"(?<=SYSTEM\\s).+(?=\\:)")

# need to take transpose
# observations are zones, parameters are select hours
apcluster::apcluster(negDistMat(r=2), a)
apres1 <- apcluster::apcluster(negDistMat(r=2), a)

# illustrative purposes only
# use apcluster's plot(apdata, data) scatterplot overlay
# need to alter t(a) so variables have better names
a
apcluster::plot(apres1, a)

# use apcluster::aggExCluster() for agglomerative dendogram
aggres1 <- aggExCluster(negDistMat(r=2), a)
plot(aggres1)

```


### make nice graphic export

```{r eval=FALSE, include=FALSE}
# set resolution
png("3var_cluster.png", units = "in", width = 8, height = 5, res = 500)

apcluster::plot(apres1, a)

dev.off()
```




# End

<br><br><br>